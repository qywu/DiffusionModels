{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87ec9ac-cb88-4908-9f66-3a9a396e1eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from UNet import UNet\n",
    "from SimpleUNet import SimpleUNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d9c047-805c-4a94-a60d-7ed2b8881b30",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c152c2-d2e5-4df0-a3e0-b4a5090f0b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "batch_size = 128\n",
    "eval_batch_size = 256\n",
    "learning_rate = 2e-4\n",
    "num_epochs = 10\n",
    "num_warmup_stesp = 100\n",
    "\n",
    "# diffusion model\n",
    "diffusion_steps = 1000\n",
    "beta_large = 0.02\n",
    "beta_small = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7c7fa-c6df-4db6-96c6-3b3accb14508",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14056262-da0c-43a0-8142-a67ac13debed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#       torchvision.datasets.MNIST('/tmp/', train=True, download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                  torchvision.transforms.Pad(2)\n",
    "#                                # torchvision.transforms.Resize(32),\n",
    "#                              ])),\n",
    "#       batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#      torchvision.datasets.MNIST('/tmp/', train=False, download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                 torchvision.transforms.Pad(2)\n",
    "#                                # torchvision.transforms.Resize(32),\n",
    "#                              ])),\n",
    "#       batch_size=eval_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e110eea3-c652-470a-a03d-878b0f653d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffSet(Dataset):\n",
    "    def __init__(self, train, dataset=\"MNIST\"):\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "        datasets = {\n",
    "            \"MNIST\": MNIST,\n",
    "            \"Fashion\": FashionMNIST,\n",
    "            \"CIFAR\": CIFAR10,\n",
    "        }\n",
    "\n",
    "        train_dataset = datasets[dataset](\n",
    "            \"./data\", download=True, train=train, transform=transform\n",
    "        )\n",
    "\n",
    "        self.dataset_len = len(train_dataset.data)\n",
    "\n",
    "        if dataset == \"MNIST\" or dataset == \"Fashion\":\n",
    "            pad = transforms.Pad(2)\n",
    "            data = pad(train_dataset.data)\n",
    "            data = data.unsqueeze(3)\n",
    "            self.depth = 1\n",
    "            self.size = 32\n",
    "        elif dataset == \"CIFAR\":\n",
    "            data = torch.Tensor(train_dataset.data)\n",
    "            self.depth = 3\n",
    "            self.size = 32\n",
    "        self.input_seq = ((data / 255.0) * 2.0) - 1.0\n",
    "        self.input_seq = self.input_seq.moveaxis(3, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.input_seq[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba1b3390-7078-48d7-bb39-a66a3b3fc011",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DiffSet(True, \"MNIST\")\n",
    "val_dataset = DiffSet(False, \"MNIST\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "test_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7488919e-7f19-44ac-a50f-aa4d3708cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "# model = UNet(\n",
    "#         input_channels=1,\n",
    "#         input_height=32,\n",
    "#         ch=64,\n",
    "#         ch_mult=(1, 2, 2, 2),\n",
    "#         num_res_blocks=1,\n",
    "#         attn_resolutions=(16,),\n",
    "#         resamp_with_conv=True,\n",
    "#         dropout=0.,\n",
    "#         )\n",
    "\n",
    "model = SimpleUNet(img_depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6bc48a-c963-4597-820e-3995053ed6b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tests Model and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "878fc1ee-40bf-42f8-955d-8e2ba0b5f7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "img = batch[0][0].view(32,32)\n",
    "fig = plt.figure\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4d78b91-fa31-4fe4-888b-a0328864d170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ = torch.randn(1, 1, 32, 32)\n",
    "t = torch.zeros(1)\n",
    "output = model(x_, t)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6584bf6e-332e-4e3f-99b8-bf9e55a8b382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parameters\n",
    "sum(p.numel() for p in model.parameters()) // 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74728582-7a3e-44a4-baa4-a9d7d8953c9e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9fada49-72d8-4dd5-a56d-533c835a1e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_noise(imgs):\n",
    "#     z_noise = torch.randn_like(imgs)\n",
    "#     ts = torch.randint(0, diffusion_steps, (imgs.shape[0],), device=imgs.device)\n",
    "#     a_sampled = alpha_bars[ts].view(-1, 1, 1, 1)\n",
    "#     noised_imgs = torch.sqrt(a_sampled) * imgs + torch.sqrt(1 - a_sampled) * z_noise\n",
    "#     return noised_imgs, ts, z_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1e005bc-22f1-4b44-8871-a895a1ef1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficietns\n",
    "betas = torch.Tensor([beta_small + (t / diffusion_steps) * (beta_large - beta_small) for t in range(diffusion_steps)])\n",
    "alphas = 1 - betas\n",
    "alpha_bars = torch.cumprod(alphas, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb2ac224-a978-422f-9bb4-5493e76dc97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_stesp, num_training_steps=len(train_loader)*num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "999d6778-9284-4009-b59b-df65759111f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to device\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "betas = betas.to(device)\n",
    "alphas = alphas.to(device)\n",
    "alpha_bars = alpha_bars.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48cd9a94-26d9-4055-9592-0b891a2f9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta(t):\n",
    "    return beta_small + (t / diffusion_steps) * (\n",
    "        beta_large - beta_small\n",
    "    )\n",
    "\n",
    "def alpha(t):\n",
    "    return 1 - beta(t)\n",
    "\n",
    "def alpha_bar(t):\n",
    "    return math.prod([alpha(j) for j in range(t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b625605d-b033-441c-a07b-0e032ad26570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(imgs):\n",
    "    \n",
    "    ts = torch.randint(0, diffusion_steps, [imgs.shape[0]], device=imgs.device)\n",
    "    noise_imgs = []\n",
    "    epsilons = torch.randn(imgs.shape, device=imgs.device)\n",
    "    for i in range(len(ts)):\n",
    "        a_hat = alpha_bar(ts[i])\n",
    "        noise_imgs.append(\n",
    "            (math.sqrt(a_hat) * imgs[i]) + (math.sqrt(1 - a_hat) * epsilons[i])\n",
    "        )\n",
    "    noise_imgs = torch.stack(noise_imgs, dim=0)\n",
    "    return noise_imgs, ts, epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae64c641-94ed-4f18-b4de-72b19819f787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fff44b42c1a4d739e158585ea07b55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_229389/4121448459.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    pbar = tqdm.auto.tqdm(train_loader)\n",
    "    moving_loss = 0.0\n",
    "    for batch in pbar:\n",
    "        # imgs, labels = batch\n",
    "        imgs = batch\n",
    "        imgs = imgs.to(device)\n",
    "        # imgs = imgs * 2 - 1\n",
    "        \n",
    "        noised_imgs, ts, z_noise = add_noise(imgs)\n",
    "        \n",
    "        e_hat = model(noised_imgs, ts)\n",
    "        \n",
    "        loss = nn.functional.mse_loss(\n",
    "            e_hat.reshape(imgs.shape[0], -1), z_noise.reshape(imgs.shape[0], -1)\n",
    "        )\n",
    "        \n",
    "        moving_loss = moving_loss * 0.9 + 0.1 * loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        pbar.set_postfix({'loss': moving_loss})\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        with torch.random.fork_rng():\n",
    "            torch.random.manual_seed(123)\n",
    "            loss = 0\n",
    "            for batch in tqdm.auto.tqdm(test_loader):\n",
    "                # imgs, labels = batch\n",
    "                imgs = batch\n",
    "                imgs = imgs.to(device)\n",
    "                # imgs = imgs * 2 - 1\n",
    "\n",
    "                noised_imgs, ts, z_noise = add_noise(imgs)\n",
    "\n",
    "                e_hat = model(noised_imgs, ts)\n",
    "\n",
    "                loss += nn.functional.mse_loss(\n",
    "                    e_hat.reshape(imgs.shape[0], -1), z_noise.reshape(imgs.shape[0], -1)\n",
    "                )\n",
    "\n",
    "            loss /= len(test_loader)\n",
    "            print(f\"Epoch {epoch} Validation loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c99dea-8b4d-444f-a83a-15ed8770c95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659cf787-19a7-4bec-bd3c-ffc49b54640d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce20555-c6a0-4354-a9dd-f053ef6bbdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24b84d-8449-413b-9aca-0ef6f58f5974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42664e5a-d3c0-42ad-a93c-362e138ccb51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404257fc-640d-46c9-b8df-ea0c6991a08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95753069-f01d-49e4-aa8e-9ddbcc3c0738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177042ed-8f94-4342-a547-ae0954fbdf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d1d8c-4373-4d71-8399-fa419ffe9849",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Corresponds to Algorithm 1 from (Ho et al., 2020).\n",
    "\"\"\"\n",
    "ts = torch.randint(0, self.t_range, [batch.shape[0]], device=self.device)\n",
    "noise_imgs = []\n",
    "epsilons = torch.randn(batch.shape, device=self.device)\n",
    "for i in range(len(ts)):\n",
    "    a_hat = self.alpha_bar(ts[i])\n",
    "    noise_imgs.append(\n",
    "        (math.sqrt(a_hat) * batch[i]) + (math.sqrt(1 - a_hat) * epsilons[i])\n",
    "    )\n",
    "noise_imgs = torch.stack(noise_imgs, dim=0)\n",
    "e_hat = self.forward(noise_imgs, ts.unsqueeze(-1).type(torch.float))\n",
    "loss = nn.functional.mse_loss(\n",
    "    e_hat.reshape(-1, self.in_size), epsilons.reshape(-1, self.in_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8522025-9f44-4bd6-aa12-33fa2719e243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3fa39b-97ee-4f82-bae0-552cde058ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcfe03e-2569-4936-bb3f-52d1936237cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba2c3a-fb1a-42f5-a346-0afc1e05079b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca522a-0ce9-483c-8cd7-528c3582cf29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a72290-ab7a-493b-b39c-5ee48fdb8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process samples and save as gif\n",
    "gen_samples = (gen_samples * 255).type(torch.uint8)\n",
    "gen_samples = gen_samples.reshape(-1, gif_shape[0], gif_shape[1], train_dataset.size, train_dataset.size, train_dataset.depth)\n",
    "\n",
    "def stack_samples(gen_samples, stack_dim):\n",
    "    gen_samples = list(torch.split(gen_samples, 1, dim=1))\n",
    "    for i in range(len(gen_samples)):\n",
    "        gen_samples[i] = gen_samples[i].squeeze(1)\n",
    "    return torch.cat(gen_samples, dim=stack_dim)\n",
    "\n",
    "gen_samples = stack_samples(gen_samples, 2)\n",
    "gen_samples = stack_samples(gen_samples, 2)\n",
    "\n",
    "imageio.mimsave(\n",
    "    f\"{trainer.logger.log_dir}/pred.gif\",\n",
    "    list(gen_samples),\n",
    "    fps=5,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
